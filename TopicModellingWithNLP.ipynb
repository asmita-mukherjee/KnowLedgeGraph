{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad2d3ef-a1cf-41e8-80fe-aaf7ef0321f2",
   "metadata": {},
   "source": [
    "## INTRO\n",
    "\n",
    "Topic modelling of movie plots using LDA. \n",
    "LDA will be performed on Bag of words representation and TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e20afc7e-7ae5-4a73-9750-0050ffe8f364",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94547029-90cb-482e-8b78-453c9662c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9742ee9-d42a-4a78-8ed2-0a198101196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a document into a list of tokens ignoring tokens that are either too short or too long\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd6efe41-e80d-4da5-b6e5-841f3b4e3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea720dd8-e005-48ce-bf7c-c62e3ee818d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer #for lemmatization and stemming respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8497da-6422-4ed6-9425-2cd7e4c628c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0222c33f-b5ef-4f23-b2ac-85dc94294759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5568f09-6242-4d1f-a0de-a74797fce52e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc9855dd-e579-4147-a667-1392e7d34ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dec370-9e2c-4250-bf3a-7c06d6f45bad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8ef831-d84e-4e17-be69-dfc8b3dda117",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_the_data = 'Data\\wiki_movie_plots_deduped.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cbca59-4411-48eb-b0a8-0f75cd60c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_to_the_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392eb44a-9367-4563-9431-7cb0a2a69471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34886 entries, 0 to 34885\n",
      "Data columns (total 8 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Release Year      34886 non-null  int64 \n",
      " 1   Title             34886 non-null  object\n",
      " 2   Origin/Ethnicity  34886 non-null  object\n",
      " 3   Director          34886 non-null  object\n",
      " 4   Cast              33464 non-null  object\n",
      " 5   Genre             34886 non-null  object\n",
      " 6   Wiki Page         34886 non-null  object\n",
      " 7   Plot              34886 non-null  object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07569d-b855-4b74-80d7-6244c52f2d47",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1) Break each sentence into tokens.\n",
    "\n",
    "2) Tokens of size < 3 is removed since they might be prepositions , articles and abbreviations\n",
    "\n",
    "3) StopWords removed (Stopwords are words that occur in larger frequency in sentences and often carries lesser information) (Query : Does one needs to perform stopwords removal when using TF-IDF ? )\n",
    "\n",
    "4) Words are lemmatized so as to make the changes in the form of the words dues to tenses and parts of speeches , uniform\n",
    "\n",
    "5) Words are stemmed so as to reduce them to their root form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6935ceec-8808-46e7-956a-e0bdb73968b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_stemmed_lemmatized_token(token):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(language = 'english')\n",
    "    \n",
    "    lemmatized_token = lemmatizer.lemmatize(token, pos='v')\n",
    "    \n",
    "    stemmed_token = stemmer.stem(lemmatized_token)\n",
    "    \n",
    "    return stemmed_token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8039c46c-4e5b-4620-b16b-2f89f2fb4f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_of_processed_tokens(text):\n",
    "    ''' Given a sentence returns the list of stemmed lemmatized tokens and also eliminates tokens that are of lenght < 3'''\n",
    "    \n",
    "    list_of_tokens = []\n",
    "    \n",
    "    tokens_in_sen = gensim.utils.simple_preprocess(doc = text,min_len=4) #only returns tokens in the sen of len atleast 4\n",
    "    \n",
    "    \n",
    "    for token in tokens_in_sen :\n",
    "        \n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
    "            stemmed_lemmatized_token = return_stemmed_lemmatized_token(token)\n",
    "            \n",
    "            list_of_tokens.append(stemmed_lemmatized_token)\n",
    "    \n",
    "    \n",
    "    return list_of_tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9740c4aa-24ef-4564-9408-674ef965e821",
   "metadata": {},
   "source": [
    "## Obtaining the tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a9eb134-eb0b-4469-a348-117cc924067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_from_the_plot = dataset['Plot'].map(return_list_of_processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e598f14-4d23-4e66-92cb-7d2089315df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [bartend, work, saloon, serv, drink, custom, f...\n",
       "1    [moon, paint, smile, face, hang, park, night, ...\n",
       "2    [film, minut, long, compos, shot, girl, sit, b...\n",
       "3    [last, second, consist, shot, shoot, wood, win...\n",
       "4    [earliest, know, adapt, classic, fairytal, fil...\n",
       "Name: Plot, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_from_the_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b42e8-02a4-4c1b-a75a-b1d0e5a1bdda",
   "metadata": {
    "tags": []
   },
   "source": [
    "There you go!!!! , the plots are converted to lemmatized and stemmed tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6359f0-fa74-4a77-a039-a48d4a589c6e",
   "metadata": {},
   "source": [
    "## Building Dictonary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ca5b4e3-2a0e-4732-b2b2-03d8a6c65d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary_of_unique_words = gensim.corpora.Dictionary(tokens_from_the_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469d7cc2-e436-4935-8159-2cc2ce430035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34886"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictonary_of_unique_words.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "116ad8af-dfa0-482f-a8b2-fd2020e536d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove tokens that occur in less than 10 movie plots\n",
    "#remove tokens that appear in more than 50% of the movie plots (these might not provide any useful information regarding the topic of the movie)\n",
    "\n",
    "#Once the above steps are completed the keep_n parameter ensures that we keep the most frequent 100000 tokens (i.e we do not consider tokens which occur in very less frequncy amongst movie plots)\n",
    "\n",
    "# note : no_below would remove the tokens which occurs in less number of documents , it does not consider the overall frequency of the token which is taken into account by keep_n\n",
    "dictonary_of_unique_words.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafb371-6eca-499e-b200-ac7a3ee3fd3a",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "A matrix where row is each document (i.e the plot in this case) and the columns contain all the unique words in the vocabulary , each cell indicates the frequency of each word in the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96de4e3f-cbba-4841-a7d8-59aafd0293a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim doc2bow returnd a list of tupples of form (id of the word , freq of the word in the doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4b9a6b0-13aa-4660-8c4f-33fe958451c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictonary_of_unique_words.doc2bow(each_plot_token) for each_plot_token in tokens_from_the_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "662cd6f5-bd80-4fd5-affc-e5a1293943f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(42, 1),\n",
       " (48, 1),\n",
       " (139, 1),\n",
       " (172, 2),\n",
       " (173, 1),\n",
       " (223, 1),\n",
       " (254, 1),\n",
       " (264, 1),\n",
       " (308, 1),\n",
       " (315, 1),\n",
       " (319, 1),\n",
       " (416, 1),\n",
       " (457, 1),\n",
       " (523, 1),\n",
       " (532, 1),\n",
       " (587, 1),\n",
       " (657, 1),\n",
       " (707, 2),\n",
       " (767, 1),\n",
       " (771, 1),\n",
       " (882, 3),\n",
       " (1120, 1),\n",
       " (1180, 1),\n",
       " (1371, 1),\n",
       " (1688, 1),\n",
       " (2019, 2),\n",
       " (2046, 1),\n",
       " (2211, 1),\n",
       " (2220, 1),\n",
       " (2221, 1),\n",
       " (2222, 1),\n",
       " (2223, 1),\n",
       " (2224, 1),\n",
       " (2225, 4),\n",
       " (2226, 1),\n",
       " (2227, 6),\n",
       " (2228, 1),\n",
       " (2229, 1),\n",
       " (2230, 1),\n",
       " (2231, 1),\n",
       " (2232, 1),\n",
       " (2233, 1),\n",
       " (2234, 1),\n",
       " (2235, 1),\n",
       " (2236, 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[100] # for any particular plot it contains the frequncy of the words from vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044169-bccc-4c58-9bf7-38d9347939e4",
   "metadata": {},
   "source": [
    "# TF IDF method of vectorizing sentences \n",
    "\n",
    "TF IDF handles the issues with N grams where in order to reduce the space requirement we might often eliminate words which occur rarely in documents\n",
    "\n",
    "TF IDF increases the value associated with a word if it occurs multiple times in a single document and decreases the value if it occues in multiple documents (it might be stop word in that case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76cc10a9-b572-45b2-9860-d657f7e3e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8e46255-30c6-49be-9e3d-e2271a7a1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = models.TfidfModel(bow_corpus) # fitting the tf - idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e12bfebd-dd9d-403c-81a0-4e759fdb57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the tf idf represnetation of each document\n",
    "\n",
    "corpus_tfidf = tf_idf_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30518a3-8a31-4f39-96d4-2398fc8fc07e",
   "metadata": {},
   "source": [
    "## Using LDA to do topic modelling of movie plots\n",
    "\n",
    "LDA is used to categorize when the number of attributes/dimensions of the dataset is large. \n",
    "LDA acheives this by minimizing the variance/seperation between each classes and maximizing the seperation between the means of each class\n",
    "\n",
    "applying LDA on tfidf and bag of words representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9465dd65-6197-4fc9-b194-aeb383ac2c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7e8bcba-fa91-45d3-91ca-73e56e7367d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing LDA\n",
    "\n",
    "# corpus = vectorized result\n",
    "#id2Word = uses this to understand word to id mapping in the corpus . Note above , the corpus only contains the ID of the words and not the words itself\n",
    "#workers = Since we are using the multiprocessing version of the LDA implementation from gensim , this represents the number of threads\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus = corpus_tfidf, num_topics=4, id2word=dictonary_of_unique_words,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1dfb1bcf-ca60-46e9-b79c-b11146a5096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(model,num_words):\n",
    "    for idx, topic in model.show_topics(num_words=5): # prints the most important topics\n",
    "        print('Topic: {} Words & Probability: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b06df9bf-1d8b-455c-b5df-11f6e51993df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words & Probability: 0.002*\"film\" + 0.002*\"kill\" + 0.001*\"life\" + 0.001*\"villag\" + 0.001*\"stori\"\n",
      "Topic: 1 Words & Probability: 0.002*\"vijay\" + 0.002*\"love\" + 0.002*\"villag\" + 0.002*\"kill\" + 0.002*\"father\"\n",
      "Topic: 2 Words & Probability: 0.002*\"love\" + 0.002*\"marri\" + 0.002*\"raju\" + 0.002*\"father\" + 0.002*\"famili\"\n",
      "Topic: 3 Words & Probability: 0.002*\"famili\" + 0.002*\"father\" + 0.002*\"love\" + 0.001*\"mother\" + 0.001*\"girl\"\n"
     ]
    }
   ],
   "source": [
    "show_topics(lda_model_tfidf,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b2afea0-4494-4162-8347-db6072c7dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bow = gensim.models.LdaMulticore(corpus = bow_corpus, num_topics=4, id2word=dictonary_of_unique_words,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1d393dd9-cd1d-435d-b03a-cf47b49b2dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Words & Probability: 0.008*\"kill\" + 0.006*\"leav\" + 0.005*\"take\" + 0.004*\"tell\" + 0.004*\"return\"\n",
      "Topic: 1 Words & Probability: 0.006*\"father\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"tell\" + 0.004*\"friend\"\n",
      "Topic: 2 Words & Probability: 0.006*\"love\" + 0.006*\"kill\" + 0.005*\"famili\" + 0.005*\"tell\" + 0.005*\"take\"\n",
      "Topic: 3 Words & Probability: 0.007*\"love\" + 0.007*\"tell\" + 0.006*\"leav\" + 0.005*\"kill\" + 0.005*\"father\"\n"
     ]
    }
   ],
   "source": [
    "show_topics(lda_model_bow,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c80267-5c97-470a-be09-ad2a62d8bb4d",
   "metadata": {
    "tags": []
   },
   "source": [
    " #### Thus we cleaely observe the differences in categories or topic for bag of model and TF IDF representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b251a97-78b6-4567-a6b1-c5e54c53cd75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing and Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7324ba-c6ad-4976-975f-1ba27973a471",
   "metadata": {
    "tags": []
   },
   "source": [
    "Random Sampling for Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c88eebc-6362-47a8-a4c4-f6896ec6d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tokens_from_the_plot[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5b5d0550-3511-42a4-9c4e-d09534b85987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bag of words representation for the sample\n",
    "bow_sample = dictonary_of_unique_words.doc2bow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f36e1401-8aa6-4962-a8af-b032973122ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.11183456), (3, 0.88134974)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_bow[bow_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c8714a8-ee95-40d4-93d2-3d1f8269b9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score :0.9063881039619446\t \n",
      "Topic0.007*\"love\" + 0.007*\"tell\" + 0.006*\"leav\" + 0.005*\"kill\" + 0.005*\"father\" + 0.005*\"marri\" + 0.005*\"time\" + 0.005*\"come\" + 0.005*\"friend\" + 0.004*\"go\"\n",
      "\n",
      " Score :0.08679026365280151\t \n",
      "Topic0.006*\"love\" + 0.006*\"kill\" + 0.005*\"famili\" + 0.005*\"tell\" + 0.005*\"take\" + 0.004*\"friend\" + 0.004*\"leav\" + 0.004*\"go\" + 0.004*\"help\" + 0.004*\"return\"\n"
     ]
    }
   ],
   "source": [
    "# observe the score of each class\n",
    "\n",
    "for index,score in sorted(lda_model_bow[bow_sample],reverse=True):\n",
    "    print('\\n Score :{}\\t \\nTopic{}'.format(score, lda_model_bow.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d828c-3077-49e6-974e-c12edd7d9dab",
   "metadata": {},
   "source": [
    "Random Sampling with Tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aead6304-f219-4f09-9bef-b25e6895362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tf idf vector representation for the sample\n",
    "tf_idf_sample = tf_idf_model[bow_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6219f4f-ec13-4ecd-8373-247f60c6f51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6495862), (1, 0.03226512), (2, 0.28528637), (3, 0.032862287)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf[tf_idf_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f83bbfb1-3895-4f55-a483-4ea5b7503cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score :0.03289150819182396\t \n",
      "Topic0.002*\"famili\" + 0.002*\"father\" + 0.002*\"love\" + 0.001*\"mother\" + 0.001*\"girl\" + 0.001*\"wife\" + 0.001*\"marri\" + 0.001*\"film\" + 0.001*\"friend\" + 0.001*\"kill\"\n",
      "\n",
      " Score :0.21758344769477844\t \n",
      "Topic0.002*\"love\" + 0.002*\"marri\" + 0.002*\"raju\" + 0.002*\"father\" + 0.002*\"famili\" + 0.001*\"stori\" + 0.001*\"arjun\" + 0.001*\"brother\" + 0.001*\"marriag\" + 0.001*\"film\"\n",
      "\n",
      " Score :0.03227980062365532\t \n",
      "Topic0.002*\"vijay\" + 0.002*\"love\" + 0.002*\"villag\" + 0.002*\"kill\" + 0.002*\"father\" + 0.002*\"polic\" + 0.001*\"murder\" + 0.001*\"marri\" + 0.001*\"famili\" + 0.001*\"ajay\"\n",
      "\n",
      " Score :0.7172452211380005\t \n",
      "Topic0.002*\"film\" + 0.002*\"kill\" + 0.001*\"life\" + 0.001*\"villag\" + 0.001*\"stori\" + 0.001*\"young\" + 0.001*\"famili\" + 0.001*\"girl\" + 0.001*\"father\" + 0.001*\"year\"\n"
     ]
    }
   ],
   "source": [
    "# observe the score of each class\n",
    "\n",
    "for index,score in sorted(lda_model_tfidf[tf_idf_sample],reverse=True):\n",
    "    print('\\n Score :{}\\t \\nTopic{}'.format(score, lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd970a8-f269-4a0d-b9b7-8a361332cce4",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Strangely BOW performs better in sample 23 in order to categorize the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08343341-df3a-4526-9230-ab840e5331ba",
   "metadata": {},
   "source": [
    "## Unseen Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "93df2262-7ad2-41e8-9b2c-b6b6a07f4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define unseen text\n",
    "\n",
    "unseen_document = \"The main charecter runs out of the house and tells his friend to get some help\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ea76d12-47ec-4cf6-913c-59a955f8f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the bag of words representation for the sample\n",
    "bow_sample = dictonary_of_unique_words.doc2bow(return_list_of_processed_tokens(unseen_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cea7cef-8545-430c-bbe2-3e0d539eabc4",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ca8f336a-4d75-4cf4-a344-92f0fcd371a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score :0.8845025897026062\t \n",
      "Topic0.007*\"love\" + 0.007*\"tell\" + 0.006*\"leav\" + 0.005*\"kill\" + 0.005*\"father\" + 0.005*\"marri\" + 0.005*\"time\" + 0.005*\"come\" + 0.005*\"friend\" + 0.004*\"go\"\n",
      "\n",
      " Score :0.03881495073437691\t \n",
      "Topic0.006*\"love\" + 0.006*\"kill\" + 0.005*\"famili\" + 0.005*\"tell\" + 0.005*\"take\" + 0.004*\"friend\" + 0.004*\"leav\" + 0.004*\"go\" + 0.004*\"help\" + 0.004*\"return\"\n",
      "\n",
      " Score :0.03852028772234917\t \n",
      "Topic0.006*\"father\" + 0.006*\"kill\" + 0.006*\"leav\" + 0.005*\"tell\" + 0.004*\"friend\" + 0.004*\"go\" + 0.004*\"take\" + 0.004*\"meet\" + 0.004*\"year\" + 0.004*\"come\"\n",
      "\n",
      " Score :0.03816216066479683\t \n",
      "Topic0.008*\"kill\" + 0.006*\"leav\" + 0.005*\"take\" + 0.004*\"tell\" + 0.004*\"return\" + 0.004*\"friend\" + 0.004*\"go\" + 0.004*\"film\" + 0.003*\"father\" + 0.003*\"come\"\n"
     ]
    }
   ],
   "source": [
    "# observe the score of each class\n",
    "\n",
    "for index,score in sorted(lda_model_bow[bow_sample],reverse=True):\n",
    "    print('\\n Score :{}\\t \\nTopic{}'.format(score, lda_model_bow.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d6cb1-9bf1-4237-86d8-c4f6dbf8a879",
   "metadata": {},
   "source": [
    "Clearly the LDA on Bag of words categorizes the unseen document to topic 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52467e-01aa-4f86-95c5-605ce2ef824b",
   "metadata": {},
   "source": [
    "Tf IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8480dc4b-6a87-4870-8c25-e0def49aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tf idf vector representation for the sample\n",
    "tf_idf_sample = tf_idf_model[bow_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ff05cfe-1792-4bfb-89aa-8614fd5ab6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Score :0.7449244260787964\t \n",
      "Topic0.002*\"famili\" + 0.002*\"father\" + 0.002*\"love\" + 0.001*\"mother\" + 0.001*\"girl\" + 0.001*\"wife\" + 0.001*\"marri\" + 0.001*\"film\" + 0.001*\"friend\" + 0.001*\"kill\"\n",
      "\n",
      " Score :0.08547670394182205\t \n",
      "Topic0.002*\"love\" + 0.002*\"marri\" + 0.002*\"raju\" + 0.002*\"father\" + 0.002*\"famili\" + 0.001*\"stori\" + 0.001*\"arjun\" + 0.001*\"brother\" + 0.001*\"marriag\" + 0.001*\"film\"\n",
      "\n",
      " Score :0.08388233184814453\t \n",
      "Topic0.002*\"vijay\" + 0.002*\"love\" + 0.002*\"villag\" + 0.002*\"kill\" + 0.002*\"father\" + 0.002*\"polic\" + 0.001*\"murder\" + 0.001*\"marri\" + 0.001*\"famili\" + 0.001*\"ajay\"\n",
      "\n",
      " Score :0.08571653068065643\t \n",
      "Topic0.002*\"film\" + 0.002*\"kill\" + 0.001*\"life\" + 0.001*\"villag\" + 0.001*\"stori\" + 0.001*\"young\" + 0.001*\"famili\" + 0.001*\"girl\" + 0.001*\"father\" + 0.001*\"year\"\n"
     ]
    }
   ],
   "source": [
    "# observe the score of each class\n",
    "\n",
    "for index,score in sorted(lda_model_tfidf[tf_idf_sample],reverse=True):\n",
    "    print('\\n Score :{}\\t \\nTopic{}'.format(score, lda_model_tfidf.print_topic(index,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402b705-8240-4662-9d51-8eecc508720b",
   "metadata": {},
   "source": [
    "The TF-IDF too categorizes the unkonw sentence to topic 0 but with lesser probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a16ef-2883-45de-a79a-cef94a3eef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
